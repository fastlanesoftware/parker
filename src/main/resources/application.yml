# Confluent Kafka configuration
spring:
  kafka:
    properties:
      sasl.mechanism: PLAIN
      sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule   required username='${kafkaUser}'   password='${kafkaPassword}';
      bootstrap.servers: pkc-ldvj1.ap-southeast-2.aws.confluent.cloud:9092
      security.protocol: SASL_SSL
      # client.dns.lookup: use_all_dns_ips
      # Set to 10 seconds, so brokers are not overwhelmed in cases of incorrectly configured or expired credentials
      reconnect.backoff.max.ms: 10000
      # Keep this default for cloud environments
      # request.timeout.ms: 30000
      # client.id: ${siteName}-bv-app
      application.id: demo-group
      # Best practice for Kafka producer to prevent data loss
    acks: all
# setup the producer queues
spring.cloud.stream.bindings:
  rerun-request:
    destination: demo-request
    producer:
      useNativeEncoding: true
  login-activity:
    destination: demo-activity
    producer:
      useNativeEncoding: true

# setup the consumer queues
spring.cloud:
  function:
    definition: tanalystInvoiceConsumer;rerunCompletedConsumer
  stream.bindings:
    tanalystInvoiceConsumer-in-0:
      destination: demo-received
    rerunCompletedConsumer-in-0:
      destination: demo-completed

# setup the consumer configuration
spring.cloud.stream.kafka.streams.binder:
  configuration:
    default.key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
    default.value.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
    commit.interval.ms: 1000


management:
  endpoint:
    health:
      show-details: ALWAYS
  endpoints:
    web:
      exposure:
        include: metrics,health,bindings,kafkastreamstopology
